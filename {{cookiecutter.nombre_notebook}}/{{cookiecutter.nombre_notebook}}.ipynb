{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: Antes de usar el notebook, asegurase que se tiene seleccionado el `kernel` llamado `Python 3.9.13`.\n",
    "\n",
    "NOTEBOOK creado por medio del comando `cookiecutter gh:centraal-api/plantilla-cientificos-ciudadanos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalación de la librerias.\n",
    "# La primera vez en ejecutarse demora unos minutos.\n",
    "%pip install -U setuptools wheel\n",
    "%pip install azure-datalake-utils\n",
    "%pip install pandas-profiling\n",
    "%pip install flaml==1.0.12\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinciar el kernel mediante la opción de VScode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, se va abrir un navegador, el cualquier va requerir autenticación con el directorio activo de Haceb. Por favor usar las credenciales con las que acceden a aplicativos como `office365`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from azure_datalake_utils import Datalake\n",
    "from flaml import AutoML\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "# configuración del datalake.\n",
    "DATALAKENAME = \"{{cookiecutter.nombre_datalake}}\"\n",
    "dl = Datalake(DATALAKENAME, \"{{cookiecutter.tenant}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {{cookiecutter.nombre_notebook}}\n",
    "\n",
    "Autora/Autor: {{cookiecutter.nombre_autor}}\n",
    "\n",
    "Correo: {{cookiecutter.correo_electronico}}\n",
    "\n",
    "Area: {{cookiecutter.area}}\n",
    "\n",
    "{{cookiecutter.descripcion_corta_notebook}}\n",
    "\n",
    "El notebook tiene las siguiente secciones, estas son una sugerencia para mantener organizados todos los proyectos.\n",
    "\n",
    "1. Lectura de archivos desde el datalake.\n",
    "2. Exploración de datos.\n",
    "3. Transformación de datos (incluyendo escritura hacia el datalake).\n",
    "4. Entrenamiento y validación de modelos.\n",
    "5. Generación de predicciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos desde el datalake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar que en el datalake tenemos dos tipos de contenedores/carpetas:\n",
    " \n",
    "- Contenedores curados: Se pueden reconocer por que tienen la palabra `curated` en su nombre. Algunos ejemplos: `hacebanalitica-curated-calidad`, `hacebanalitica-curated-servicio`. Estos contenedores tienen las siguiente reglas:\n",
    "    - Solo es posible leer información y no escribir.\n",
    "    - Cualquier modificación que se necesite en los archivos, debe ser coordinada con el equipo de arquitectura.\n",
    "- Contenedores de usuario: Se pueden reconcer por que tiene la palabra `user` en su nombre. Algunos ejemplos: `hacebanalitica-user-calidad`, `hacebanalitica-user-cientificos`. Estos contenedores tienen las siguiente reglas:\n",
    "    - Es posible leer y escribir información.\n",
    "    - La área correspondiente (ejemplo `calidad`), es la dueña de la información y pueden definir los cambios necesarios en coordinación con el equipo correspondiente.\n",
    "    - Si se encuentran en un proceso de experimentación, tratar de seguir el siguiente orden de prioridad:\n",
    "        - Usar el contenedor del área, ejemplo si el proyecto es de `calidad`, usar el contenedor `hacebanalitica-user-calidad`\n",
    "        - Si no se tiene disponibilidad del contenedor del area, usar `hacebanalitica-user-cientificos`. En futuros avances requerir al equipo de TI/Arquitectura la creación de un contenedor de usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informacion_base = dl.read_csv(\"hacebanalitica-user-cientificos/prueba/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informacion_excel = dl.read_excel(\"hacebanalitica-user-cientificos/prueba/diabetes.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura avanzada por particion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se necesita leer datos \"crudos\" que se encuentran particionados, la libreria `azure-datalake-utils` tiene funcionalidades para facilitar ese trabajo. Un archivo particionado tiene la ventajas de:\n",
    "1. reducir los tiempos de procesamiento en la lectura\n",
    "2. filtrar solo la información que se necesaria\n",
    "3. reducir la carga de memoria ram de la maquina local.\n",
    "\n",
    "La libreria solo soporta particiones tipo `hive`, una partición tipo se reconoce porque las carpetas dentro del datalake siguen la siguiente estructura `path/to/archivo/col=valor/col=valor/archivo.csv`. Un ejemplo mas concreto seria el siguiente `raw/tuya_cloud/device_logs/product_name=Nevera Himalaya Smart 448/start_date=2022_10_23`, donde existen dos columnas de partición `product_name` y `start_date`.\n",
    "\n",
    "Esta funcionalidad **solo** funciona cuando el dl se crea con el metodo `from_account_key`. Por buena practica **NO inlcuir keys dentro del notebook**, se sugiere crear un archivo `.json` con el contenido del key:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key\" : \"valor del key\"\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dateutil.relativedelta import relativedelta\n",
    "with open(\"key.json\", 'r') as f:\n",
    "    key = json.loads(f.read())['key']\n",
    "\n",
    "dl = Datalake.from_account_key(DATALAKENAME, key)\n",
    "\n",
    "# leer todos la informacion existente del product name Nevera Himalaya Smart 448.\n",
    "# de las ultimas dos semanas\n",
    "now = pd.Timestamp.now().date() - relativedelta(days=1)\n",
    "fechas_a_cargar = [c.strftime(\"%Y_%m_%d\") for c in pd.date_range(start = now - relativedelta(days=15), end = now)]\n",
    "\n",
    "df = dl.read_csv_with_partition(ruta = \"hacebanalitica/raw/tuya_cloud/device_logs/\", \n",
    "    partition_inclusion =  {'product_name' : ['Nevera Himalaya Smart 448'] , 'start_date':fechas_a_cargar},\n",
    "    sep = \"|\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "\n",
    "Es cualquier experimento o proyecto, es ideal dejar en evidencia la descripción de los archivos que se van a usar. Se sugiere que esta exploración se realice usando la libreria [Pandas Profiling](https://pandas-profiling.ydata.ai/docs/master/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(informacion_base, title=\"Exploracion de datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de ver el reporte, por favor condensar las principales conclusiones de la exploración de datos, enfocarse en:\n",
    "\n",
    "1. Calidad de datos:\n",
    "    1. ¿Hay variables con valores faltantes?\n",
    "\n",
    "    [escribir propias conclusiones]\n",
    "\n",
    "    2. ¿Hay variables con valores atípicos?\n",
    "\n",
    "    [escribir propias conclusiones]\n",
    "\n",
    "2. Tendencias:\n",
    "    1. ¿Hay variables que tengan tendencias, ejemplo valores que se repiten mucho, o muy cercanos?\n",
    "\n",
    "    [escribir propias conclusiones]\n",
    "\n",
    "    2. ¿ en que variables hay alta correlación?\n",
    "    \n",
    "\n",
    "3. Patrones insuales:\n",
    "    1. En problemas de clasificación, ¿hay clases que tengan más muestras que otras?\n",
    "\n",
    "    [escribir propias conclusiones]\n",
    "\n",
    "    2. En problema de regresión, ¿hay meses o epocas del año que tengan más valores?\n",
    "\n",
    "    [escribir propias conclusiones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de datos\n",
    "\n",
    "En esta sección se deben aplicar las transformaciones que hayan a lugar. En lo posible solo usar operaciones de [Pandas](https://pandas.pydata.org/).\n",
    "\n",
    "Se muestran algunos ejemplos que sirven de inspiración, pero se sugiere explorar mucho más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover duplicados.\n",
    "informacion_base_dedup = informacion_base.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar valores mediante mapeo\n",
    "mapeo = {\n",
    "    0 : 'setosa',\n",
    "    1: 'versicolor',\n",
    "    2: 'virginica'\n",
    "}\n",
    "\n",
    "informacion_base['clase_nombre'] = informacion_base['class'].map(mapeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar columnas.\n",
    "informacion_excel.rename(columns = {'bp': 'more_human_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizar variables.\n",
    "informacion_excel['progression_clase'] = pd.qcut(informacion_excel['progression'], \n",
    "    q = 5 , \n",
    "    labels = ['baja', 'media-baja', 'media', 'media-alta', 'alta'])\n",
    "\n",
    "informacion_excel[['progression_clase', 'progression']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer muestreo.\n",
    "informacion_base.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables dummy.\n",
    "pd.get_dummies(informacion_base, columns = ['clase_nombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables dummy usando sklearn.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=True)\n",
    "onehot = enc.fit_transform(informacion_base[['clase_nombre']])\n",
    "#to print the encoded features for train data\n",
    "pd.DataFrame(onehot, columns=enc.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones sin cambiar la estructura.\n",
    "informacion_base['media sepal width (cm)'] = informacion_base.groupby(['class'])['sepal width (cm)'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones cambiando la estructura\n",
    "informacion_base_agg = informacion_base.groupby(['class'], as_index = False)['sepal width (cm)'].mean()\n",
    "informacion_base_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar dos dataframes.\n",
    "informacion_base_duplicada = pd.concat([informacion_base, informacion_base], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mezclar dataframes.\n",
    "informacion_base_merge = informacion_base.merge(informacion_base_agg, on = ['class'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tablas pivote.\n",
    "pivote = pd.pivot_table(informacion_excel, values = ['age', 'sex'], index = ['progression_clase'], aggfunc='sum')\n",
    "pivote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer Melt de dataframes.\n",
    "df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                   'B': {0: 1, 1: 3, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})\n",
    "pd.melt(df, id_vars=['A'], value_vars=['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y validación de modelos\n",
    "\n",
    "En esta sección concentrarse en entrenar y validar el modelo. Dentro de esta sección se sugiere:\n",
    "\n",
    "1. Realizar la división de datos, conservar al menos un 10% de datos que no se usaran en el entrenamiento.\n",
    "2. Realizar entrenamiento usando herramientas AutoML. De esta manera la selección de parametros y modelos sera más eficiente. En la plantilla se sugiere usar las siguientes librerias:\n",
    "    1. [Fast Library for Automated Machine Learning & Tunning](https://microsoft.github.io/FLAML/).\n",
    "En un futuro se debe analizar otras librerias como [AutoGluon](https://auto.gluon.ai/stable/index.html) y [Auto-Sklearn](https://automl.github.io/auto-sklearn/master/). Por el momento por garantizar la estabilidad en el proceso, no es posible ofrecer un uso.\n",
    "\n",
    "En las siguiente secciones se muestran algunos ejemplos, para inspiarar el uso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# seperar las caracteristcas\n",
    "caracteristicas = ['sepal length (cm)', \n",
    "                   'sepal width (cm)', \n",
    "                   'petal length (cm)',\n",
    "                   'petal width (cm)']\n",
    "target = 'class'\n",
    "X = informacion_base[caracteristicas].copy()\n",
    "y = informacion_base[target].copy()\n",
    "# mantener random_state para reproducibilidad.\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Advertencia!: Si el problema que se esta desarollando el tiempo es importante se debe usar un criterio de limites de fechas, ejemplo si se tiene un datos de tres años, se debe separar los ultimos meses para realizar la prueba. Usar las funciones de pandas para aplicar los filtros correspondientes. Un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limite_entrenamiento = '2017-09-01'\n",
    "entrenamiento = informacion_excel[informacion_excel['fecha']<limite_entrenamiento].copy()\n",
    "prueba = informacion_excel[informacion_excel['fecha']>=limite_entrenamiento].copy()\n",
    "#\n",
    "caracteristicas2 =['age', 'sex', 'bmi', 'bp', 's1', 's2']\n",
    "target2 = 'progression'\n",
    "X_train2 = entrenamiento[caracteristicas2].copy()\n",
    "y_train2 = entrenamiento[target2].copy()\n",
    "# \n",
    "X_test2 = entrenamiento[caracteristicas2].copy()\n",
    "y_test2 = entrenamiento[target2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificación FLAML. \n",
    "automl1 = AutoML()\n",
    "automl1.fit(X_train, y_train, task=\"classification\", time_budget = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saber las sugerencias de AUTOML.\n",
    "print(automl1.best_estimator)\n",
    "print(automl1.best_config)\n",
    "print(automl1.best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener el mejor modelo.\n",
    "mejor_modelo_1 = automl1.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validar el mejor modelo.\n",
    "# usar la metrica más adecuada desde sklearn.\n",
    "from  sklearn import metrics\n",
    "y_pred = mejor_modelo_1.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión con FLAML.\n",
    "automl2 = AutoML()\n",
    "automl2.fit(X_train, y_train, task=\"regression\", time_budget = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saber las sugerencias de AUTOML.\n",
    "print(automl2.best_estimator)\n",
    "print(automl2.best_config)\n",
    "print(automl2.best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener el mejor modelo.\n",
    "mejor_modelo_2 = automl2.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validar el mejor modelo.\n",
    "# usar la metrica más adecuada desde sklearn.\n",
    "from  sklearn import metrics\n",
    "y_pred2 = mejor_modelo_2.predict(X_test2)\n",
    "print(metrics.mean_squared_error(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de predicciones\n",
    "\n",
    "Una vez se tenga el modelo seleccionado, las predicciones deben recibir las caracteristicas. Se sugiere a esas caracteristicas agregar como columna el valor predicho y guardar esas predicciones dentro del datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_a_predecir = pd.DataFrame(\n",
    "    {\n",
    "        'sepal length (cm)' : [4.6\t, 3.3, 4.1],\n",
    "        'sepal width (cm)' : [3.5, 3.6, 2.5],\n",
    "        'petal length (cm)' : [1.4, 5.4, 2.3],\n",
    "        'petal width (cm)' : [0.2, 2.3, 1.9],\n",
    "    }\n",
    ")\n",
    "base_a_predecir['class'] = mejor_modelo_1.predict(base_a_predecir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.write_excel(base_a_predecir, \"hacebanalitica-user-cientificos/prueba/iris_pred.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.write_csv(base_a_predecir, \"hacebanalitica-user-cientificos/prueba/iris_pred.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b145243b2ff1142fe66038d6c3bd724d42c01f344fdc8044c68da453fb6fddb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
